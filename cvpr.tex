% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[review]{cvpr}
%\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

%\usepackage[ruled,noend,linesnumbered]{algorithm2e}
\usepackage{graphicx,color,psfrag}
\usepackage{amsfonts}
\usepackage{bigdelim}
\usepackage{dsfont}
\usepackage{color, soul}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cite}
\usepackage{dsfont}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2023}
%\setcounter{page}{4321} % For final version only


\begin{document}

%%%%%%%%% TITLE
\title{Hand-Object Pose Estimation from RGBD Images}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle


%%%%%%%%% ABSTRACT
\begin{abstract}
Hand-object pose estimation aims to predict the pose and shape of both the hand and held object under interaction. Although numerous applications in the real world such as augmented and virtual reality, hand-object pose estimation concerns relatively less attention. Several methods separately estimate hand shapes and object poses but totally neglect the correlations between hands and objects. In this work, we introduce an approach that leverages the advantage of the voting mechanism to jointly learn the appearance of hands and objects from RGB-D images. We propose a module called adaptive fusion to effectively collaborate RGB and Depth features by adaptively adding the weight for each type of feature before fusing. The output features can discriminate the differently meaningful distribution between color and depth information at each position. Moreover, we embrace the graph convolutional network (GCN) to learn the interaction relationships between the hand and held object shapes under manipulation. Unlike conventional methods looking at the center points of the hand and objects, our approach takes into account the hand and object keypoints that belong to their surfaces to estimate the shapes and learn the correlations. This facilitates examining the geometric constraints and spatial restrictions to achieve accurate outcomes. Experiments using benchmark datasets illustrate that our network achieves beyond state-of-the-art accuracy in 3D pose estimation.
\end{abstract}

%%%%%%%%% BODY TEXT

%
\input{Introduction}
%
\input{RelatedWork}
%
\input{Method}
%
\input{Evaluation}
%
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
