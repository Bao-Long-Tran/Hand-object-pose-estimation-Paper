\section{Related work}
\label{sec:relatedwork}

\subsection{Hand-object pose estimation}
The naive approach for the problem is treat the hand \cite{yang2022dynamic, madadi2017end, deng2017hand3d, oberweger2017deepprior++, iqbal2018hand} and manipulated object \cite{wang2019densefusion, schwarz2015rgb, qi2018frustum, zhou2018voxelnet} seperately without considering their interdependence. They underestimate the extraordinary relationship between the hand gestures and object shapes. Several approaches overcome this problem by jointly learn the shapes of both hand and object from RGB images. \cite{doosti2020hope} develops two graph convolutional networks for two missions. The first one detects 2D hand joints and 2D object corners, while the second one lifts 2D keypoints to 3D coordinates. \cite{tse2022collaborative} proposes attention-guided graph convolution to iteratively share hand and object estimator between two branches for learning the mutual occlusion. \cite{hasson2020leveraging} looks into photometric consistency between neighboring frames to reconstruct hand-object shape under interactions. \cite{tekin2019h+} handles hand action classification to assist the process of estimating hand-object interactions. \cite{liu2021semi} introduce a semi-supervised learning framework leveraging spatial-temporal consistency to improve estimation performance. However, the absence of depth information makes the process of learning physical constraints and interactions latent. In addition, the transformation from 2D to 3D world is difficultl to accurately proceed due to high degree of non-linearity. In contrary, some methods exploit solely depth images. \cite{choi2017robust} designs an architecture that firstly predicts hand and object centers and then learn global orientations and grasps of hand configurations while interacting with objects. \cite{zhang2021single, goudie20173d} segments 2D hand and object regions from depth image and then optimizes the reconstruction process of interacting motions. This method, however, learns the depth images by 2D CNN backbone hence cannot radically observe the geometric information. \cite{oberweger2019generalized} deploys a feedback loop to revise the flawed estimation results using depth images only. RGB-D input data, on the other hand, has received relatively less attention due to how to effectively collaborate two distinctive input format still holds a secret. \cite{kyriazis2013physically} focuses on the physical laws of hand actions from RGB-D input to benefit hand-object interaction interpretations. \cite{tsoli2018joint} tracks hands and objects in dealing with a complex scenario in which manipulated objects are deformable. 

\subsection{RGB-D fusion}
With the common of color-depth camera, a wide range of computer vision research such as object segmentation \cite{chen2021global, chen2020bi, park2017rdfnet, zhang2021non} and 6D object detection \cite{wang2019densefusion, tian2020robust, saadi2021optimizing} has been inspired to learn and incorporate color and depth features from RGB-D images. The RGB image and depth image belong to different modalities, so most fusing feature methods are \cite{wang2021brief}: image layer fusion, feature layer fusion, and output layer fusion. While image layer fusion concates the input data before feeding to CNNs, feature layer fusion means learning color and depth data in two distinguished architecture but sharing the learning process. Output layer fusion, on the other hand, integrate two feature maps which are seperately extracted by two backbone networks. However, fusion RGB-D features for hand-object pose estimation is less attractive because  most of mentioned methods have the mutual weak point that is extracting features from depth maps by 2D CNNs. This makes the output 3D spatial feature is latent and unconcious. Motivated by \cite{wang2019densefusion}, we develop a network of feature layer fusion that can share learning process between two backbones but can export geometric information and geometric contraints by using Pointnet++ \cite{qi2017pointnet++} backbone for the depth map. In addition, our network adaptively and selectively adjusts features at each pixel before fusing to gain the accurate outcome.

\subsection{Voting mechanism for pose estimation}
The Hough voting is originally introduced to detect 2D defined shapes \cite{hough1959machine, duda1972textordfeminineuse} and then developed to futher complex computer vision tasks \cite{silberberg1984iterative, tombari2010object}. The deep learning-based voting methods \cite{ding2019votenet, kehl2016deep, wu2021vote, hoang2022voting} have recently appeared to be a promising approach for object detection and pose estimation due to its robustness and ability to novel object. 