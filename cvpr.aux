\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{cai20203d,zimmermann2017learning,gao2019variational,xiang2017posecnn,tremblay2018deep}
\citation{oberweger2017deepprior++,moon2018v2v,xiong2019a2j,ge20173d,cai2022ove6d,li2020category}
\citation{kazakos2018fusion,yuan20193d}
\citation{hasson2019learning,hasson2020leveraging,tekin2019h+,tse2022collaborative,liu2021semi,oikonomidis2011full,lu2021understanding,hasson2021towards,wang2020learning}
\citation{choi2017robust,zhang2021single,goudie20173d,oberweger2019generalized}
\citation{kyriazis2013physically,tsoli2018joint}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{\hskip -1em.~Introduction}{section.1}{}}
\@writefile{brf}{\backcite{cai20203d, zimmermann2017learning, gao2019variational, xiang2017posecnn, tremblay2018deep}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{oberweger2017deepprior++, moon2018v2v, xiong2019a2j, ge20173d, cai2022ove6d, li2020category}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{kazakos2018fusion, yuan20193d}{{1}{1}{section.1}}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:result_exp}{{\caption@xref {fig:result_exp}{ on input line 9}}{1}{\hskip -1em.~Introduction}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Our method captures features from both RGB and depth images (a) to vote for hand and object keypoints. (b) The hand point cloud and voting vectors (blue lines), each point on the hand (green point) votes for 21 MANO joints (red points). (c) The held object and voting vectors (blue lines), each point on the same object (green point) votes for $M=9$ selected keypoints (red points). The interaction relationships between the hand and the held object are learn by feeding voting vectors into a GCN. (d) The hand-object pose estimation result.\relax }}{1}{figure.caption.1}}
\citation{wang2019densefusion}
\citation{yang2022dynamic,madadi2017end,deng2017hand3d,oberweger2017deepprior++,iqbal2018hand}
\citation{wang2019densefusion,schwarz2015rgb,qi2018frustum,zhou2018voxelnet}
\citation{hasson2020leveraging}
\citation{tekin2019h+}
\citation{liu2021semi}
\citation{choi2017robust}
\citation{zhang2021single,goudie20173d}
\citation{oberweger2019generalized}
\citation{kyriazis2013physically}
\citation{tsoli2018joint}
\@writefile{brf}{\backcite{hasson2019learning, hasson2020leveraging, tekin2019h+, tse2022collaborative, liu2021semi, oikonomidis2011full, lu2021understanding, hasson2021towards, wang2020learning}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{choi2017robust, zhang2021single, goudie20173d, oberweger2019generalized}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{kyriazis2013physically, tsoli2018joint}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{wang2019densefusion}{{2}{1}{figure.caption.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related work}{2}{section.2}}
\newlabel{sec:relatedwork}{{2}{2}{\hskip -1em.~Related work}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.\nobreakspace  {}Hand-object pose estimation}{2}{subsection.2.1}}
\@writefile{brf}{\backcite{yang2022dynamic, madadi2017end, deng2017hand3d, oberweger2017deepprior++, iqbal2018hand}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{wang2019densefusion, schwarz2015rgb, qi2018frustum, zhou2018voxelnet}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{hasson2020leveraging}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{tekin2019h+}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{liu2021semi}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{choi2017robust}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{zhang2021single, goudie20173d}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{oberweger2019generalized}{{2}{2.1}{subsection.2.1}}}
\citation{chen2021global,chen2020bi,park2017rdfnet,zhang2021non}
\citation{wang2019densefusion,tian2020robust,saadi2021optimizing}
\citation{wang2021brief}
\citation{wang2019densefusion}
\citation{qi2017pointnet++}
\citation{kong2020sia}
\citation{doosti2020hope}
\citation{almadani2021graph}
\citation{tse2022collaborative}
\citation{wang2019densefusion}
\citation{wang2019densefusion}
\citation{wang2019densefusion}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Overview of our proposal}. Our method takes both color and depth maps as input data. The color features are extracted by a CNN, while the 3D features are calculated by PointNet++ architecture. These two types of features are then fused at a pixel level to obtain the new distinctive features by the adaptive fusion network. This network learns to predict the weight matrix that facilitates beneficial features to eclipse the tedious information. We design a deep Hough voting-based network to vote for 21 MANO hand joints and $M=9$ selected object keypoints. A GCN-based network is deployed to learn the constraints and interdependencies between the hand and object poses to boost the estimation performance.\relax }}{3}{figure.caption.2}}
\newlabel{fig:Hand_pose}{{2}{3}{\textbf {Overview of our proposal}. Our method takes both color and depth maps as input data. The color features are extracted by a CNN, while the 3D features are calculated by PointNet++ architecture. These two types of features are then fused at a pixel level to obtain the new distinctive features by the adaptive fusion network. This network learns to predict the weight matrix that facilitates beneficial features to eclipse the tedious information. We design a deep Hough voting-based network to vote for 21 MANO hand joints and $M=9$ selected object keypoints. A GCN-based network is deployed to learn the constraints and interdependencies between the hand and object poses to boost the estimation performance.\relax }{figure.caption.2}{}}
\@writefile{brf}{\backcite{kyriazis2013physically}{{3}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{tsoli2018joint}{{3}{2.1}{subsection.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.\nobreakspace  {}RGB-D fusion}{3}{subsection.2.2}}
\@writefile{brf}{\backcite{chen2021global, chen2020bi, park2017rdfnet, zhang2021non}{{3}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{wang2019densefusion, tian2020robust, saadi2021optimizing}{{3}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{wang2021brief}{{3}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{wang2019densefusion}{{3}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{qi2017pointnet++}{{3}{2.2}{subsection.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\hskip -1em.\nobreakspace  {}Graph convolutional network for pose estimation}{3}{subsection.2.3}}
\@writefile{brf}{\backcite{kong2020sia}{{3}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{doosti2020hope}{{3}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{almadani2021graph}{{3}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{tse2022collaborative}{{3}{2.3}{subsection.2.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Method}{3}{section.3}}
\newlabel{sec:methodology}{{3}{3}{\hskip -1em.~Method}{section.3}{}}
\citation{qi2017pointnet}
\citation{qi2017pointnet++}
\citation{romero2022embodied}
\citation{kong2020sia,doosti2020hope}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Adaptive fusion.} Illustration of our proposal adaptive network in comparison with Dense fusion \cite  {wang2019densefusion}. The network takes color feature (a) and depth feature (b) as input and learns their diverse distribution of contributing meaningfulness across all positions. Whereas, Dense fusion values RGB and depth feature at each pixel equally. The white and pink columns' height denotes the important degree of each type of feature at each pixel. Intuitively, our output feature (c) is more densely informative than dense fusion output (d).\relax }}{4}{figure.caption.3}}
\@writefile{brf}{\backcite{wang2019densefusion}{{4}{3}{figure.caption.3}}}
\newlabel{fig:adaptive_fusion}{{3}{4}{\textbf {Adaptive fusion.} Illustration of our proposal adaptive network in comparison with Dense fusion \cite {wang2019densefusion}. The network takes color feature (a) and depth feature (b) as input and learns their diverse distribution of contributing meaningfulness across all positions. Whereas, Dense fusion values RGB and depth feature at each pixel equally. The white and pink columns' height denotes the important degree of each type of feature at each pixel. Intuitively, our output feature (c) is more densely informative than dense fusion output (d).\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Adaptive Fusion}{4}{subsection.3.1}}
\newlabel{sec:adaptive_fusion}{{3.1}{4}{\hskip -1em.~Adaptive Fusion}{subsection.3.1}{}}
\@writefile{brf}{\backcite{wang2019densefusion}{{4}{3.1}{figure.caption.3}}}
\@writefile{brf}{\backcite{qi2017pointnet}{{4}{3.1}{figure.caption.3}}}
\@writefile{brf}{\backcite{qi2017pointnet++}{{4}{3.1}{figure.caption.3}}}
\newlabel{eq:fusion}{{1}{4}{\hskip -1em.~Adaptive Fusion}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Hand and Object Voting}{4}{subsection.3.2}}
\newlabel{sec:voting}{{3.2}{4}{\hskip -1em.~Hand and Object Voting}{subsection.3.2}{}}
\@writefile{brf}{\backcite{romero2022embodied}{{4}{3.2}{subsection.3.2}}}
\newlabel{eq:loss_handjoints}{{2}{4}{\hskip -1em.~Hand and Object Voting}{equation.3.2}{}}
\@writefile{brf}{\backcite{kong2020sia, doosti2020hope}{{4}{3.2}{equation.3.2}}}
\bibstyle{ieee_fullname}
\bibdata{egbib}
\bibcite{almadani2021graph}{1}
\newlabel{eq:loss_objectpoints}{{3}{5}{\hskip -1em.~Hand and Object Voting}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Hand and Object Poses Estimation}{5}{subsection.3.3}}
\newlabel{sec:interaction}{{3.3}{5}{\hskip -1em.~Hand and Object Poses Estimation}{subsection.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Illustration of the interaction learning between votes for hand keypoints and votes for object keypoints. The red points denote hand keypoints, while the yellow ones denote object keypoints. The blue and green vectors represent hand keypoints and object keypoints votes, respectively.\relax }}{5}{figure.caption.4}}
\newlabel{eq:edge_feature}{{4}{5}{\hskip -1em.~Hand and Object Poses Estimation}{equation.3.4}{}}
\newlabel{eq:loss_handpose}{{5}{5}{\hskip -1em.~Hand and Object Poses Estimation}{equation.3.5}{}}
\newlabel{eq:loss_handpose}{{6}{5}{\hskip -1em.~Hand and Object Poses Estimation}{equation.3.6}{}}
\newlabel{eq:hand-object}{{8}{5}{\hskip -1em.~Hand and Object Poses Estimation}{equation.3.8}{}}
\bibcite{cai2022ove6d}{2}
\bibcite{cai20203d}{3}
\bibcite{chen2021global}{4}
\bibcite{chen2020bi}{5}
\bibcite{choi2017robust}{6}
\bibcite{deng2017hand3d}{7}
\bibcite{doosti2020hope}{8}
\bibcite{gao2019variational}{9}
\bibcite{ge20173d}{10}
\bibcite{goudie20173d}{11}
\bibcite{hasson2020leveraging}{12}
\bibcite{hasson2021towards}{13}
\bibcite{hasson2019learning}{14}
\bibcite{iqbal2018hand}{15}
\bibcite{kazakos2018fusion}{16}
\bibcite{kong2020sia}{17}
\bibcite{kyriazis2013physically}{18}
\bibcite{li2020category}{19}
\bibcite{liu2021semi}{20}
\bibcite{lu2021understanding}{21}
\bibcite{madadi2017end}{22}
\bibcite{moon2018v2v}{23}
\bibcite{oberweger2017deepprior++}{24}
\bibcite{oberweger2019generalized}{25}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Evaluation}{6}{section.4}}
\newlabel{sec:Eval }{{4}{6}{\hskip -1em.~Evaluation}{section.4}{}}
\bibcite{oikonomidis2011full}{26}
\bibcite{park2017rdfnet}{27}
\bibcite{qi2018frustum}{28}
\bibcite{qi2017pointnet}{29}
\bibcite{qi2017pointnet++}{30}
\bibcite{romero2022embodied}{31}
\bibcite{saadi2021optimizing}{32}
\bibcite{schwarz2015rgb}{33}
\bibcite{tekin2019h+}{34}
\bibcite{tian2020robust}{35}
\bibcite{tremblay2018deep}{36}
\bibcite{tse2022collaborative}{37}
\bibcite{tsoli2018joint}{38}
\bibcite{wang2021brief}{39}
\bibcite{wang2019densefusion}{40}
\bibcite{wang2020learning}{41}
\bibcite{xiang2017posecnn}{42}
\bibcite{xiong2019a2j}{43}
\bibcite{yang2022dynamic}{44}
\bibcite{yuan20193d}{45}
\bibcite{zhang2021non}{46}
\bibcite{zhang2021single}{47}
\bibcite{zhou2018voxelnet}{48}
\bibcite{zimmermann2017learning}{49}
